# ğŸ¨ Diffusion2 - Minimal Diffusion Model Implementation

A clean, educational implementation of a **diffusion-based generative model** from scratch, featuring training, sampling, and a FastAPI REST API for inference.

Right now Diffusion is not Diffusing
---

## ğŸ“‹ Table of Contents

- [Features](#-features)
- [Project Structure](#-project-structure)
- [Quick Start](#-quick-start)
- [Installation](#-installation)
- [Usage](#-usage)
  - [Training](#training-the-model)
  - [Sampling](#sampling-generation)
  - [API Server](#serving-via-fastapi)
- [Configuration](#%EF%B8%8F-configuration)
- [Architecture](#-architecture)
- [Dependencies](#-dependencies)

---

## âœ¨ Features

âœ… **From-Scratch Implementation** â€“ UNet-based diffusion model without third-party frameworks  
âœ… **YAML Configuration** â€“ Centralized, tunable training & model hyperparameters  
âœ… **FastAPI Integration** â€“ RESTful API endpoint for image generation  
âœ… **Docker Support** â€“ Containerized deployment ready  
âœ… **Caltech-101 Dataset** â€“ Pre-configured for object image training  
âœ… **GPU/CPU Aware** â€“ Automatic device detection and fallback  
âœ… **Modular Architecture** â€“ Clean separation of config, model, diffusion, and utilities  

---

## ğŸ“ Project Structure

```
Diffusion2/
â”œâ”€â”€ ğŸ“„ README.md                    # This file
â”œâ”€â”€ ğŸ“„ main.py                      # Entry point for CLI modes (train, sample, serve)
â”œâ”€â”€ ğŸ“„ train_collab.ipynb           # notebook for experimentation
â”œâ”€â”€ ğŸ“„ pyproject.toml               # Project metadata & dependencies
â”œâ”€â”€ ğŸ³ Dockerfile                   # Container image definition
â”œâ”€â”€ ğŸ“¦ .python-version              # Python version spec (3.x)
â”œâ”€â”€ ğŸ“¦ .venv/                       # Virtual environment (gitignored)
â”œâ”€â”€ ğŸ“‚ build/                       # build outputs (wheel, egg-info)
â”œâ”€â”€ ğŸ“‚ diffusion2.egg-info/         # package metadata (top-level)
â”‚
â”œâ”€â”€ ğŸ“‚ app/
â”‚   â””â”€â”€ app.py                      # FastAPI application & HTTP endpoints
â”‚
â”œâ”€â”€ ğŸ“‚ configs/
â”‚   â””â”€â”€ base.yaml                   # Hyperparameters & model config (YAML)
â”‚
â”œâ”€â”€ ğŸ“‚ data/
â”‚   â””â”€â”€ plane/                      # local plane images for Dataset
â”‚
â”œâ”€â”€ ğŸ“‚ Dataset/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ plane.py                    # custom dataset loader
â”‚
â”œâ”€â”€ ğŸ“‚ Logger/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ logger.py                   # logging helper
â”‚
â”œâ”€â”€ ğŸ“‚ logs/
â”‚   â”œâ”€â”€ inference-logs/
â”‚   â””â”€â”€ train-logs/
â”‚
â”œâ”€â”€ ğŸ“‚ saves/                       # trained model checkpoints
â”œâ”€â”€ ğŸ“‚ scripts/
â”‚   â””â”€â”€ temp.ipynb                 # miscellaneous script
â”‚
â”œâ”€â”€ ğŸ“‚ src/                         # installable source package
â”‚   â”œâ”€â”€ diffusion2.egg-info/        # package metadata within src
â”‚   â”œâ”€â”€ ğŸ“‚ mini_diffusion/          # Core diffusion model package
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config.py               # Config loader & Pydantic models
â”‚   â”‚   â”œâ”€â”€ diffusion.py            # Diffusion process (noise scheduling)
â”‚   â”‚   â”œâ”€â”€ model.py                # UNet architecture with time embeddings
â”‚   â”‚   â”œâ”€â”€ preprocessing.py        # image transforms
â”‚   â”‚   â”œâ”€â”€ train.py                # Training loop
â”‚   â”‚   â”œâ”€â”€ sample.py               # Sampling/inference function
â”‚   â”‚   â””â”€â”€ __pycache__/
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“‚ argparsers/              # CLI argument parsers (extensible)
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ train_parser.py
â”‚       â””â”€â”€ inference_parser.py
â”‚
â”œâ”€â”€ ğŸ“‚ tests/                        # Unit tests (currently empty)
â””â”€â”€ ğŸ“‚ data/plane/                   # dataset images
```

---

## ğŸš€ Quick Start

### 1ï¸âƒ£ **Installation**

```bash
# Clone the repository
git clone <repo-url>
cd Diffusion2

# Create and activate virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install -e .
# or manually:
pip install torch torchvision numpy pydantic pyyaml tqdm fastapi uvicorn pillow
```

### 2ï¸âƒ£ **Train the Model**

```bash
# use the configuration file to control hyperparameters & paths
uv run python main.py train --config ./configs/base.yaml
```

Trains a UNet diffusion model on the dataset specified in the config (default is Caltech-101 airplanes). A checkpoint is written to the `save_path` defined in the config (by default `./saves/a.pth`).

### 3ï¸âƒ£ **Generate Images**

```bash
# pass the same config file used for training so the model path and device are picked up
uv run python main.py sample --config ./configs/base.yaml
```

The `sample` mode will load the checkpoint configured under `inference.model_path` and run the reverse diffusion process, writing the resulting PNG to `sample.png` in the current directory. You can also prefix the command with `uv run` if you are running inside the project's UV environment:

```bash
uv run python ./main.py sample --config ./configs/base.yaml
```

This command is **for generation only**; training should still use the `train` mode.  

(Adjust paths and options in `configs/base.yaml` to point to your trained model or to change device settings.)

### 4ï¸âƒ£ **Serve via API**

The `serve` mode still starts the FastAPI server, but you can also call uvicorn directly as before:

```bash
python main.py serve
# or directly:
uvicorn app.app:app --reload --host 0.0.0.0 --port 8000
```

API is now live at `http://localhost:8000`

**Endpoints:**
- `GET /` â€“ Health check
- `GET /generate` â€“ Generate and return PNG image

```bash
# Example: Download generated image
curl -s http://localhost:8000/generate --output generated.png
```

---

## âš™ï¸ Configuration

Edit `configs/base.yaml` to tune hyperparameters:

```yaml
model:
  image_size: 28              # Input/output image resolution
  in_channels: 1              # 1 for grayscale, 3 for RGB
  base_channels: 64           # Base channel count for UNet

diffusion:
  timesteps: 1000             # Number of noise steps in diffusion
  beta_start: 0.0001          # Noise schedule start
  beta_end: 0.02              # Noise schedule end

training:
  batch_size: 64              # Batch size per iteration
  epochs: 10                  # Number of training epochs
  learning_rate: 1e-4         # AdamW optimizer LR
  device: "cuda"              # "cuda" or "cpu"
```

---

## ğŸ—ï¸ Architecture

### **UNet Model** (model.py)

- **Sinusoidal Time Embedding** â€“ Encodes timestep `t` as positional embeddings
- **Encoder (Down-sampling)** â€“ 2 convolutional down-blocks with max-pooling
- **Bottleneck** â€“ Central processing block
- **Decoder (Up-sampling)** â€“ 2 transposed convolutions with skip connections
- **Output** â€“ Predicts noise to subtract from noisy input

### **Diffusion Process** (diffusion.py)

- **Forward** â€“ Adds noise to images: $x_t = \sqrt{\bar{\alpha}_t} x_0 + \sqrt{1-\bar{\alpha}_t} \epsilon$
- **Backward** â€“ Iteratively denoises: $x_{t-1} = \frac{1}{\sqrt{\alpha_t}} \left( x_t - \frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}} \hat{\epsilon}_\theta(x_t, t) \right)$

### **Training** (train.py)

- Loads Caltech-101 dataset with preprocessing
- Samples random timesteps and adds noise
- Minimizes L2 loss on noise prediction
- Saves trained UNet to `./saves/a.pth`

### **Sampling** (sample.py)

- Loads trained model
- Starts from random Gaussian noise
- Iteratively denoises over 1000 timesteps
- Returns final generated image

---

## ğŸ“Š Dependencies

| Library       | Purpose                                      |
|---------------|----------------------------------------------|
| `torch`       | Deep learning framework                      |
| `torchvision` | Computer vision utilities + Caltech-101 data |
| `numpy`       | Numerical computing                          |
| `pydantic`    | Config validation & type hints               |
| `pyyaml`      | YAML configuration parsing                   |
| `pillow`      | Image I/O for API responses                  |
| `fastapi`     | REST API framework                           |
| `uvicorn`     | ASGI server                                  |
| `tqdm`        | Progress bars                                |

See `pyproject.toml` for version specs.

---

## ğŸ³ Docker Deployment

Build and run in a container:

```bash
# Build image
docker build -t diffusion2 .

# Run container
docker run --gpus all -p 8000:8000 diffusion2 python main.py serve
```

---

## ğŸ“ Notes

- **First Run** â€“ Caltech-101 dataset (~130 MB) auto-downloads on first training
- **GPU Required** â€“ Training on CPU is slow; CUDA strongly recommended
- **Model Checkpoint** â€“ Trained model saved to `./saves/a.pth` (reused by sampling & API)
- **Empty Dirs** â€“ `/scripts` and `/tests` are placeholders for future expansion

---

## ğŸ”§ Troubleshooting

| Issue | Solution |
|-------|----------|
| `No module named 'mini_diffusion'` | Run `pip install -e .` from repo root |
| CUDA out of memory | Reduce `batch_size` in `configs/base.yaml` |
| Dataset download fails | Manual download: [Caltech-101](http://www.vision.caltech.edu/datasets/) |
| API port already in use | Change port: `uvicorn app.app:app --port 8001` |

---

## ğŸ“š References

- [Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2006.11239) â€“ Ho et al., 2020
- [Improved Denoising Diffusion Probabilistic Models](https://arxiv.org/abs/2102.09672) â€“ Nichol & Dhariwal, 2021
- [Caltech-101 Dataset](http://www.vision.caltech.edu/datasets/caltech101.html)

---

## ğŸ‘¤ Author

**Shubham**

---

## ğŸ“„ License

-
---

**Happy Diffusing! ğŸ¨**
